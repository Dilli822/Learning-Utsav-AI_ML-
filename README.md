# Learning Utsav Challenge - 30-Day Self Learning Journey

### Dilli Hang Rai  
üìÖ Starts from: Oct 3, 2024  
üîó **Twitter**:https://x.com/dilli_hangrae

---

## üöÄ Introduction

Welcome to my **30-Day Self Learning Challenge** as part of the **Learning Utsav Challenge**, starting on Ghatasthapana and ending on Bhaitika! This initiative is designed to turn the Dashain-Tihar festive season into a time for learning and growth. Sponsored by **Evolve IT Hub Nepal** and **Programiz**, and organized by the **KEC Electronics Club** with support from student clubs across Nepal, this challenge encourages self-learning in various technical domains.

### üìö What I'm Learning:
I'll be focusing on **AI**, **Machine Learning**, and **Deep Learning** throughout the next 30 days, sharing my progress and key learnings daily on **Twitter** using the hashtag #LearningUtsav. You can follow my journey and see the progress in this repository as well.

---

## üèÜ Challenge Overview

- **Challenge Duration**: Oct 3, 2024 (Ghatasthapana) ‚Äì Nov 2, 2024 (Bhaitika)
- **Platforms**: Twitter (#LearningUtsav)  
- **Focus Areas**: AI, Machine Learning, Deep Learning

---

## üìù Daily Progress

| Day | Topic & Twitter Post 
| --- | ------------------------------------------------------------------------------------------
| 1   | Introduction to Machine Learning, https://x.com/dilli_hangrae/status/1841763188117012784
| 2   | Exploring concepts on parameters,models,vectors,data points of objects, https://x.com/dilli_hangrae/status/1842117445768839259
| 3   | Intro to Probability & Statistics, Distributions, Discrete and Continuous, PMF,PDF, Bernoulli Distribution , https://x.com/dilli_hangrae/status/1842409414663213375
| 4   | Joint Probability Distribution, PDF, Multi-variate Gaussian Distribution,Marginal Probability,Conditional Probability,AI Perspectives: Thinking and acting rationality and humanly, https://x.com/dilli_hangrae/status/1842784087825899934
| 5   | Expectations with their properties, independence, linearity,co-variance, variance,Bernoulli Cov,emprical mean and co-variance, https://x.com/dilli_hangrae/status/1843173089397612622
| 6   | Linear Algebra,Vector vs Scalars, Vector Space, Matrices, Vector Product, Span and Basis of Vector, Matrices as Linear Transformations, Representation of Graph, Dimensionality reduction using matrices,Vector Scaling, lINEARLY DEPENDENT AND independent, 2D & 3D Vectors Space, Polar and Catesian Co-ordinates, Outer Product, Inner Product, Dot Product and Hadamard Product, https://x.com/dilli_hangrae/status/1843523500608503902
| 7   | Dot Product in ANN,Eigen Vectors and Values, Eigen Decomposition, Matrix as Linear Transformation, Roles of Eigen with animation plots , https://x.com/dilli_hangrae/status/1843882103689359408
| 8   | Quick Theoretical Tour on Machine Learning - Linear and Logistic Regression with some Mathematical notations, Loss function, training and testing dataset, Knowledge representation in AI, issues in KB repsentation and methods to represent the Knowledge, Maximum Likelihood Estimation, Parameteric vs Non-Parameteric Model of Function, https://x.com/dilli_hangrae/status/1844293749322752346
|  9  | Cross-Validatios, Regularization L1 and L2,K-NN, K-Means Clustering, SVM (non-linear), AI agents, PEAS, Sensor-Actuators-Effectors, Bias-Variance Trade-off, https://x.com/dilli_hangrae/status/1844601197946437918
|  10 | Statistical Method: Finding beta0 and beta1 y = beta0 + beta1* X where X is input and predicting the outcome 0 or 1 using Statistics of logistic regression , https://x.com/dilli_hangrae/status/1844945255583580320
|  11 | Revised: Vectors, Matrices, Basis, Dependencies, Eigenvalues/Vectors, Decomposition, Statistical/Probability Concepts (PDF, PMF, Marginal Probability, Independence, Variance/Covariance, Distributions), Chain Rule, Expectation, Worked with various static datasets in Logistic Regression, https://x.com/dilli_hangrae/status/1845352394428908024
| 12  | Supervised Learning: Classification vs Regression: Linear Regression vs Sparse Regression (using less features) and Logistic Regression vs Sparse Logistic Regression and Non-Linearity Model Tree Based: Random vs XG Boosted Random Forest(building the tree sequentially each tree step by step and correct the previous error tree with the current tree) with visualizations plots, https://x.com/dilli_hangrae/status/1845706594757353833
| 13  | Z-score normalizations, transformation, differences in scales and using sparse, featurization, filling the missing values for both classification and regression, knowledge vs facts vs information vs representations, propositional as a knowledge representation method with operators and backward+forward knowledge representations methods, https://x.com/dilli_hangrae/status/1846050651559940229
|  14 | Detailed Study on Overfitting, Underfitting, Prevention of Overfitting: Cross-Validation and Test/Split Dataset, Bias-Variance TradeOff, Logistic Regression Loss Function Derivation, 
|     |
|     |
|     |
|     |
|     |
