# Learning Utsav Challenge - 30-Day Self Learning Journey

### Dilli Hang Rai  
üìÖ Starts from: Oct 3, 2024  
üîó **Twitter**:https://x.com/dilli_hangrae

---

## üöÄ Introduction

Welcome to my **30-Day Self Learning Challenge** as part of the **Learning Utsav Challenge**, starting on Ghatasthapana and ending on Bhaitika! This initiative is designed to turn the Dashain-Tihar festive season into a time for learning and growth. Sponsored by **Evolve IT Hub Nepal** and **Programiz**, and organized by the **KEC Electronics Club** with support from student clubs across Nepal, this challenge encourages self-learning in various technical domains.

### üìö What I'm Learning:
I'll be focusing on **AI**, **Machine Learning**, and **Deep Learning** throughout the next 30 days, sharing my progress and key learnings daily on **Twitter** using the hashtag #LearningUtsav. You can follow my journey and see the progress in this repository as well.

---

## üèÜ Challenge Overview

- **Challenge Duration**: Oct 3, 2024 (Ghatasthapana) ‚Äì Nov 2, 2024 (Bhaitika)
- **Platforms**: Twitter (#LearningUtsav)  
- **Focus Areas**: AI, Machine Learning, Deep Learning

---

## üìù Daily Progress

| Day | Topic & Twitter Post 
| --- | ------------------------------------------------------------------------------------------
| 1   | Introduction to Machine Learning, https://x.com/dilli_hangrae/status/1841763188117012784
| 2   | Exploring concepts on parameters,models,vectors,data points of objects, https://x.com/dilli_hangrae/status/1842117445768839259
| 3   | Intro to Probability & Statistics, Distributions, Discrete and Continuous, PMF,PDF, Bernoulli Distribution , https://x.com/dilli_hangrae/status/1842409414663213375
| 4   | Joint Probability Distribution, PDF, Multi-variate Gaussian Distribution,Marginal Probability,Conditional Probability,AI Perspectives: Thinking and acting rationality and humanly, https://x.com/dilli_hangrae/status/1842784087825899934
| 5   | Expectations with their properties, independence, linearity,co-variance, variance,Bernoulli Cov,emprical mean and co-variance, https://x.com/dilli_hangrae/status/1843173089397612622
| 6   | Linear Algebra,Vector vs Scalars, Vector Space, Matrices, Vector Product, Span and Basis of Vector, Matrices as Linear Transformations, Representation of Graph, Dimensionality reduction using matrices,Vector Scaling, lINEARLY DEPENDENT AND independent, 2D & 3D Vectors Space, Polar and Catesian Co-ordinates, Outer Product, Inner Product, Dot Product and Hadamard Product, https://x.com/dilli_hangrae/status/1843523500608503902
| 7   | Dot Product in ANN,Eigen Vectors and Values, Eigen Decomposition, Matrix as Linear Transformation, Roles of Eigen with animation plots , https://x.com/dilli_hangrae/status/1843882103689359408
| 8   | Quick Theoretical Tour on Machine Learning - Linear and Logistic Regression with some Mathematical notations, Loss function, training and testing dataset, Knowledge representation in AI, issues in KB repsentation and methods to represent the Knowledge, Maximum Likelihood Estimation, Parameteric vs Non-Parameteric Model of Function, https://x.com/dilli_hangrae/status/1844293749322752346
|  9  | Cross-Validatios, Regularization L1 and L2,K-NN, K-Means Clustering, SVM (non-linear), AI agents, PEAS, Sensor-Actuators-Effectors, Bias-Variance Trade-off, https://x.com/dilli_hangrae/status/1844601197946437918
|  10 | Statistical Method: Finding beta0 and beta1 y = beta0 + beta1* X where X is input and predicting the outcome 0 or 1 using Statistics of logistic regression , https://x.com/dilli_hangrae/status/1844945255583580320
|  11 | Revised: Vectors, Matrices, Basis, Dependencies, Eigenvalues/Vectors, Decomposition, Statistical/Probability Concepts (PDF, PMF, Marginal Probability, Independence, Variance/Covariance, Distributions), Chain Rule, Expectation, Worked with various static datasets in Logistic Regression, https://x.com/dilli_hangrae/status/1845352394428908024
| 12  | Supervised Learning: Classification vs Regression: Linear Regression vs Sparse Regression (using less features) and Logistic Regression vs Sparse Logistic Regression and Non-Linearity Model Tree Based: Random vs XG Boosted Random Forest(building the tree sequentially each tree step by step and correct the previous error tree with the current tree) with visualizations plots, https://x.com/dilli_hangrae/status/1845706594757353833
| 13  | Z-score normalizations, transformation, differences in scales and using sparse, featurization, filling the missing values for both classification and regression, knowledge vs facts vs information vs representations, propositional as a knowledge representation method with operators and backward+forward knowledge representations methods, https://x.com/dilli_hangrae/status/1846050651559940229
|  14 | Detailed Study on Overfitting, Underfitting, Prevention of Overfitting: Cross-Validation and Test/Split Dataset, Bias-Variance TradeOff, Logistic Regression Loss Function Derivation, https://x.com/dilli_hangrae/status/1846445899838595257
|  15 |Getting into CNN,parameters, kernel operator in CNN,Motivation of Convolutional Neural Networks,Edge detection using gaussian filter and noise modeled via Gaussian Probability Distribution, https://x.com/dilli_hangrae/status/1846853957127491867
|  16 | Continuing CNNs, Pooling, Max vs Average Pooling, Applying ReLu and filters in the entire image and viewing the EKG Signals filters plots, Propositional Logic vs Predicate Logic vs First-Order Predicate Logic used in Knowledge Representation, CNNs in action and reading the block steps of FCN vs CCN, channels, batch size, input-output features and size, Receptive Field, 
|| Coding parts are:
|| Load EKG Signal, Read 1000 samples of EKG data from the MIT-BIH Arrhythmia Database (record '100'),
|| Extract Signal and Time Axis: Extract the EKG signal and create a time array in seconds,
|| Apply Gaussian Smoothing: Use a Gaussian filter to reduce noise in the signal,
|| Edge Detection (Gradient): Calculate the gradient (first derivative) of the smoothed signal to detect sharp transitions (peaks),
|| Noise Reduction: Compute noise reduction by subtracting the smoothed signal from the original,
|| Visualization: Plot the original signal, smoothed signal, edge detection result, and noise reduction in a 4-plot grid,
||Sobel Edge Detection: Apply a Sobel filter to highlight sharp changes in the signal,
||Final Plot: Plot the original signal and Sobel-filtered edge detection results side by side, https://x.com/dilli_hangrae/status/1847138828794187780
|     |
|     |
|     |
